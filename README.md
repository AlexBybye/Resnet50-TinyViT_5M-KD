# ResNet50+TinyViT_5M Knowledge Distillation

---

# é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ—¨åœ¨å°†é¢„è®­ç»ƒçš„ **ResNet50** ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œé€šè¿‡**çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰**ï¼Œå°†å…¶â€œè½¯çŸ¥è¯†â€è¿ç§»åˆ°è½»é‡çº§çš„ **TinyViT_5M** å­¦ç”Ÿæ¨¡å‹ã€‚ç›®æ ‡æ˜¯åœ¨ä¿æŒè¾ƒé«˜åˆ†ç±»ç²¾åº¦çš„åŒæ—¶ï¼Œå¤§å¹…å‡å°‘å‚æ•°é‡ä¸è®¡ç®—å¼€é”€ã€‚

- **æ•™å¸ˆæ¨¡å‹**ï¼šResNet50ï¼Œ50å±‚æ·±çš„æ®‹å·®ç½‘ç»œï¼Œåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚
    
- **å­¦ç”Ÿæ¨¡å‹**ï¼šTinyViT_5Mï¼Œæ‹¥æœ‰ä»…5Må‚æ•°çš„å°å‹è§†è§‰Transformerï¼Œç»è¿‡å¿«é€Ÿè’¸é¦æ¡†æ¶é¢„è®­ç»ƒä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚
    

> ğŸ“„ å‚è€ƒæ–‡çŒ®ï¼š[Knowledge Distillation](https://arxiv.org/abs/1503.02531)ã€[ResNet](https://arxiv.org/abs/1512.03385)ã€[TinyViT](https://arxiv.org/abs/2207.10666)

---

# æ–¹æ³•

## æ•™å¸ˆæ¨¡å‹ï¼ˆResNet50ï¼‰

- ä½äº `teacher.py`ï¼ŒåŠ è½½ torchvision å®˜æ–¹é¢„è®­ç»ƒæƒé‡ã€‚
    
- ä¿®æ”¹æœ€åå…¨è¿æ¥å±‚ä»¥é€‚åº”**7ç±»åˆ†ç±»ä»»åŠ¡**ã€‚
    
- è‡ªå®šä¹‰ `CustomDataset`ï¼Œè¯»å– `data/my_dataset/train` å’Œ `val`ï¼Œå¹¶åº”ç”¨ï¼š
    
    - `Resize`ã€`RandomResizedCrop` ç­‰å¢å¼ºã€‚
        
- è®­ç»ƒç»†èŠ‚ï¼š
    
    - æŸå¤±å‡½æ•°ï¼š**CrossEntropyLoss**
        
    - ä¼˜åŒ–å™¨ï¼š**Adam**
        
    - å­¦ä¹ ç‡ç­–ç•¥ï¼š**StepLR**ï¼ˆæ¯5ä¸ªepochä¸‹é™10å€ï¼‰
        
    - è®­ç»ƒæœ€å¤š20ä¸ªepochï¼Œ**early stopping**ï¼Œpatience=3ã€‚
        
- æœ€ä¼˜æ¨¡å‹æƒé‡ä¿å­˜ä¸º `88best_teacher_model.pth`ã€‚
    
- ç”Ÿæˆå…¨è®­ç»ƒé›†å¯¹åº”çš„ logits å¹¶ä¿å­˜ä¸º `teacher_logits.pt`ã€‚
    

ğŸ‘‰ [æŸ¥çœ‹ä»£ç ](https://github.com/AlexBybye/Resnet50-TinyViT_5M-KD/blob/master/teacher.py)

---

## å­¦ç”Ÿæ¨¡å‹ï¼ˆTinyViT_5Mï¼‰

- ä½äº `tremendous_trial.py`ï¼Œä½¿ç”¨ timm åº“åˆ›å»º `tiny_vit_5m_224` æ¨¡å‹ï¼ˆdrop_rate=0.1ï¼‰ã€‚
    
- å°è£… `Distiller` ç±»è¿›è¡Œè’¸é¦è®­ç»ƒï¼š
    
    - **ç¡¬æ ‡ç­¾æŸå¤±**ï¼šCrossEntropyLossï¼ˆlabel_smoothing=0.1ï¼‰
        
    - **è½¯æ ‡ç­¾æŸå¤±**ï¼šKLDivLossï¼ˆreduction='batchmean'ï¼‰
        
    - è’¸é¦è¶…å‚æ•°ï¼š
        
        - æ¸©åº¦ `T=4.0`
            
        - è’¸é¦æƒé‡ç³»æ•° `Î±=0.7`
            
    - `index_mapping` ç¡®ä¿æ¯ä¸ªæ ·æœ¬æ­£ç¡®æ£€ç´¢æ•™å¸ˆlogitsã€‚
        
- è®­ç»ƒç»†èŠ‚ï¼š
    
    - ä¼˜åŒ–å™¨ï¼š**AdamW**
        
    - å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š**CosineAnnealingLR**
        
    - batch_size=16ï¼Œæ€»è®­ç»ƒ20ä¸ªepochã€‚
        

ğŸ‘‰ [æŸ¥çœ‹ä»£ç ](https://github.com/AlexBybye/Resnet50-TinyViT_5M-KD/blob/master/tremendous_trial.py)

---

# æ•°æ®é›†

- æ•°æ®è·¯å¾„ï¼š`data/my_dataset`
    
- ç»“æ„ï¼š
    
    - `train/`ã€`val/` ä¸‹å„æœ‰ **OK** å’Œ **NGï¼ˆå¤šç±»åˆ«ï¼‰** å­ç›®å½•ã€‚
        
    - å…±7ä¸ªç±»åˆ«ã€‚
        
- åˆ†ç±»é€»è¾‘å‚è€ƒ `classification.py`ï¼Œä½¿ç”¨ sklearn åˆ†å‰²æ•°æ®é›†ã€‚
    

ğŸ‘‰ [æŸ¥çœ‹æ•°æ®ç»„ç»‡](https://github.com/AlexBybye/Resnet50-TinyViT_5M-KD/tree/master/data)

---

# å®éªŒè®¾ç½®

## è¶…å‚æ•°è¡¨

|è¶…å‚æ•°|å€¼|
|:--|:--|
|æ•™å¸ˆå­¦ä¹ ç‡|5e-4|
|æ•™å¸ˆbatch size|32|
|å­¦ç”Ÿå­¦ä¹ ç‡|1e-4|
|å­¦ç”Ÿbatch size|16|
|è®­ç»ƒè½®æ¬¡ï¼ˆepochï¼‰|20|
|Optimizer|Adam / AdamW|
|å­¦ä¹ ç‡è°ƒåº¦å™¨|StepLR / Cosine|
|è’¸é¦æ¸©åº¦ï¼ˆTï¼‰|4.0|
|è’¸é¦ç³»æ•°ï¼ˆÎ±ï¼‰|0.7|

## è®­ç»ƒæµç¨‹

1. è¿è¡Œ `python teacher.py` è®­ç»ƒæ•™å¸ˆæ¨¡å‹å¹¶ç”Ÿæˆ `teacher_logits.pt`ã€‚
    
2. å°† `teacher_logits.pt` å’Œ `index_mapping` æ”¾ç½®åœ¨é¡¹ç›®æ ¹ç›®å½•ã€‚
    
3. è¿è¡Œ `python tremendous_trial.py`ï¼Œå¼€å§‹å­¦ç”Ÿæ¨¡å‹è’¸é¦è®­ç»ƒã€‚
    

---

# å®éªŒç»“æœ

- æ•™å¸ˆæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šè¾¾åˆ°ç†æƒ³å‡†ç¡®ç‡ï¼ˆé€šè¿‡æ§åˆ¶å°è¾“å‡º `Val Acc: xx.xx%` æŸ¥çœ‹ï¼‰ã€‚
    
- å­¦ç”Ÿæ¨¡å‹é€šè¿‡è’¸é¦åï¼Œåœ¨éªŒè¯é›†ä¸Šè¡¨ç°ç¨³å®šï¼Œæ”¶æ•›æ›²çº¿å¹³æ»‘ã€‚
    
- **æœ€ç»ˆæ•ˆæœ**ï¼šåœ¨å¤§å¹…å‹ç¼©å‚æ•°è§„æ¨¡çš„å‰æä¸‹ï¼Œä¿æŒäº†ä¼˜è‰¯çš„åˆ†ç±»æ€§èƒ½ã€‚
## å‡†ç¡®åº¦
- æ•™å¸ˆæ¨¡å‹ï¼ˆteacher.pyï¼‰:![{C8401259-0FF7-471A-A32E-E8D9911D77DC}](https://github.com/user-attachments/assets/a1760fa4-8dbb-4567-88c1-72eed52f1716)
ï¼ˆ85%-92%ï¼‰
- è’¸é¦æ¨¡å‹ï¼ˆåŸºäº86%logitsï¼‰:![12e558d0c26cdb25eb439e4c3f87522](https://github.com/user-attachments/assets/63b41bad-1b91-4eb9-b253-97c82d655f9f)

---

# ç»“è®º

- æˆåŠŸå°† ResNet50 çš„æ·±å±‚çŸ¥è¯†ï¼ˆlogitsåˆ†å¸ƒï¼‰è’¸é¦åˆ° TinyViT_5Mï¼Œè®­ç»ƒå‡º**è½»é‡ä¸”é«˜æ•ˆ**çš„å­¦ç”Ÿæ¨¡å‹ã€‚
    
- è‡ªå®šä¹‰ç´¢å¼•æ˜ å°„å’Œæ•°æ®åŠ è½½æœºåˆ¶ï¼Œä¿è¯äº†è’¸é¦æµç¨‹çš„**ä¸€è‡´æ€§å’Œé«˜æ•ˆæ€§**ã€‚
    
- è¯¥æµç¨‹åœ¨è‡ªå®šä¹‰å°å‹åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œå…¼é¡¾äº†**ç²¾åº¦ã€ä½“ç§¯å’Œæ¨ç†é€Ÿåº¦**ã€‚
    

---

# æœªæ¥å·¥ä½œ

- å°è¯•æ›´å¤š Transformer å­¦ç”Ÿæ¨¡å‹ï¼Œå¦‚ MobileViTã€Swin Transformerã€‚
    
- åœ¨å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆå¦‚ ImageNet-1kï¼‰ä¸ŠéªŒè¯è’¸é¦æ€§èƒ½ã€‚
    
- ç»“åˆå‰ªæï¼ˆPruningï¼‰ä¸é‡åŒ–ï¼ˆQuantizationï¼‰æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥å‹ç¼©æ¨¡å‹ä½“ç§¯ã€‚
    

---

# ä½¿ç”¨è¯´æ˜

## ç¯å¢ƒé…ç½®

```bash
pip install -r requirements.txt
```

---

# å‚è€ƒæ–‡çŒ®

- He, K. et al. **Deep Residual Learning for Image Recognition**. [arXiv:1512.03385](https://arxiv.org/abs/1512.03385)
    
- Wu, K. et al. **TinyViT: Fast Pretraining Distillation for Small Vision Transformers**. [arXiv:2207.10666](https://arxiv.org/abs/2207.10666)
    
- Hinton, G., Vinyals, O., & Dean, J. **Distilling the Knowledge in a Neural Network**. [arXiv:1503.02531](https://arxiv.org/abs/1503.02531)
    
- AlexBybye. **Resnet50-TinyViT_5M-KD (GitHub)**. [é¡¹ç›®é“¾æ¥](https://github.com/AlexBybye/Resnet50-TinyViT_5M-KD)
    

---

è¦ä¸è¦æˆ‘é¡ºä¾¿ä¹Ÿå¸®ä½ å‡ºä¸€ç‰ˆæ›´ã€Œä¸“ä¸šè®ºæ–‡ã€é£æ ¼ï¼ˆæ¯”å¦‚é€‚åˆæŠ•ç¨¿æˆ–è€…å†™æˆREADME.mdçš„é‚£ç§ï¼‰ï¼Ÿè¦çš„è¯å‘Šè¯‰æˆ‘ï¼ğŸš€
